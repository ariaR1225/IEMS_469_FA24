{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|█         | 101/1000 [00:05<00:45, 19.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Average Reward: 58.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  20%|██        | 201/1000 [00:27<01:09, 11.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200, Average Reward: 185.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  22%|██▏       | 221/1000 [01:20<04:43,  2.75it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 106\u001b[0m\n\u001b[1;32m    103\u001b[0m discount_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.95\u001b[39m\n\u001b[1;32m    104\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m--> 106\u001b[0m trained_policy, episode_rewards, moving_average_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_policy_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscount_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Plotting\u001b[39;00m\n\u001b[1;32m    109\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[0;32mIn[5], line 54\u001b[0m, in \u001b[0;36mtrain_policy_gradient\u001b[0;34m(env_name, num_episodes, discount_factor, learning_rate)\u001b[0m\n\u001b[1;32m     52\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(policy(state_tensor), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     53\u001b[0m dist \u001b[38;5;241m=\u001b[39m Categorical(action_probs)\n\u001b[0;32m---> 54\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     58\u001b[0m log_probs\u001b[38;5;241m.\u001b[39mappend(dist\u001b[38;5;241m.\u001b[39mlog_prob(action))\n",
      "File \u001b[0;32m~/.conda/envs/469_env/lib/python3.9/site-packages/torch/distributions/categorical.py:133\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m    131\u001b[0m     sample_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize(sample_shape)\n\u001b[1;32m    132\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[0;32m--> 133\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(image):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\"\n",
    "    image = image[35:195] # crop\n",
    "    image = image[::2,::2,0] # downsample by factor of 2\n",
    "    image[image == 144] = 0 # erase background (background type 1)\n",
    "    image[image == 109] = 0 # erase background (background type 2)\n",
    "    image[image != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return np.reshape(image.astype(np.float).ravel(), [80,80])\n",
    "\n",
    "# Policy network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Training function\n",
    "def train_policy_gradient(env_name, num_episodes, discount_factor, learning_rate):\n",
    "    env = gym.make(env_name)\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    policy = PolicyNetwork(input_dim, output_dim)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "    episode_rewards = []\n",
    "    moving_average_rewards = []\n",
    "\n",
    "    for episode in tqdm(range(num_episodes), desc=\"Training Progress\"):\n",
    "        state, _ = env.reset()\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        \n",
    "        while True:\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            action_probs = torch.softmax(policy(state_tensor), dim=0)\n",
    "            dist = Categorical(action_probs)\n",
    "            action = dist.sample()\n",
    "            \n",
    "            next_state, reward, done, _, _ = env.step(action.item())\n",
    "            \n",
    "            log_probs.append(dist.log_prob(action))\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_reward = sum(rewards)\n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # Calculate moving average\n",
    "        if len(episode_rewards) >= 100:\n",
    "            moving_avg = np.mean(episode_rewards[-100:])\n",
    "            moving_average_rewards.append(moving_avg)\n",
    "        \n",
    "        # Calculate discounted rewards\n",
    "        discounted_rewards = []\n",
    "        R = 0\n",
    "        for r in reversed(rewards):\n",
    "            R = r + discount_factor * R\n",
    "            discounted_rewards.insert(0, R)\n",
    "        discounted_rewards = torch.FloatTensor(discounted_rewards)\n",
    "        \n",
    "        # Normalize discounted rewards\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "        \n",
    "        # Calculate loss and update policy\n",
    "        loss = []\n",
    "        for log_prob, R in zip(log_probs, discounted_rewards):\n",
    "            loss.append(-log_prob * R)\n",
    "        loss = torch.stack(loss).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            tqdm.write(f\"Episode {episode + 1}, Average Reward: {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "\n",
    "    return policy, episode_rewards, moving_average_rewards\n",
    "\n",
    "# Training\n",
    "env_name = \"CartPole-v1\"\n",
    "num_episodes = 1000\n",
    "discount_factor = 0.95\n",
    "learning_rate = 0.01\n",
    "\n",
    "trained_policy, episode_rewards, moving_average_rewards = train_policy_gradient(env_name, num_episodes, discount_factor, learning_rate)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(episode_rewards, label='Episode Reward')\n",
    "plt.plot(range(99, len(episode_rewards)), moving_average_rewards, label='Moving Average (100 episodes)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title(f'Policy Gradient Learning Curve - {env_name}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluation and Visualization\n",
    "def evaluate_and_visualize_policy(env_name, policy, num_episodes=5):\n",
    "    env = gym.make(env_name, render_mode=\"human\")\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            env.render()\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            action_probs = torch.softmax(policy(state_tensor), dim=0)\n",
    "            action = torch.argmax(action_probs).item()\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "        rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode + 1} Reward: {episode_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    return rewards\n",
    "\n",
    "print(\"\\nVisualizing trained policy:\")\n",
    "evaluation_rewards = evaluate_and_visualize_policy(env_name, trained_policy)\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean_reward = np.mean(evaluation_rewards)\n",
    "std_reward = np.std(evaluation_rewards)\n",
    "print(f\"\\nMean Reward: {mean_reward:.2f}\")\n",
    "print(f\"Standard Deviation of Reward: {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "469_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
