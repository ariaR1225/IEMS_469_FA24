{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "Training Progress:   0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 116\u001b[0m\n\u001b[1;32m    113\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m    114\u001b[0m gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.99\u001b[39m\n\u001b[0;32m--> 116\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Plot results\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 80\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, policy, optimizer, num_episodes, gamma)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Use tqdm for progress tracking\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_episodes), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Progress\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 80\u001b[0m     log_probs, rewards \u001b[38;5;241m=\u001b[39m \u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     returns \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     83\u001b[0m     R \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[1], line 52\u001b[0m, in \u001b[0;36mrun_episode\u001b[0;34m(env, policy)\u001b[0m\n\u001b[1;32m     49\u001b[0m rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     curr_state \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prev_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m         state_diff \u001b[38;5;241m=\u001b[39m curr_state \u001b[38;5;241m-\u001b[39m prev_state\n",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     18\u001b[0m image[image \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m109\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# erase background (background type 2)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m image[image \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# everything else (paddles, ball) just set to 1\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mreshape(image\u001b[38;5;241m.\u001b[39mastype(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m)\u001b[38;5;241m.\u001b[39mravel(), [\u001b[38;5;241m80\u001b[39m,\u001b[38;5;241m80\u001b[39m])\n",
      "File \u001b[0;32m~/.conda/envs/469_env/lib/python3.9/site-packages/numpy/__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def preprocess(image):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\"\n",
    "    image = image[35:195] # crop\n",
    "    image = image[::2,::2,0] # downsample by factor of 2\n",
    "    image[image == 144] = 0 # erase background (background type 1)\n",
    "    image[image == 109] = 0 # erase background (background type 2)\n",
    "    image[image != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return np.reshape(image.astype(np.float32).ravel(), [80,80])\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32 * 10 * 10, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)  # Only LEFT and RIGHT actions\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return nn.Softmax(dim=-1)(x)\n",
    "\n",
    "def run_episode(env, policy):\n",
    "    state, _ = env.reset()\n",
    "    prev_state = None\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    \n",
    "    while True:\n",
    "        curr_state = preprocess(state)\n",
    "        if prev_state is not None:\n",
    "            state_diff = curr_state - prev_state\n",
    "        else:\n",
    "            state_diff = np.zeros_like(curr_state)\n",
    "        prev_state = curr_state\n",
    "        \n",
    "        state_diff = torch.FloatTensor(state_diff).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        action_probs = policy(state_diff)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        log_prob = dist.log_prob(action)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        state, reward, done, _, _ = env.step(action.item() + 2)  # +2 to map to RIGHT, LEFT actions\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return log_probs, rewards\n",
    "\n",
    "def train(env, policy, optimizer, num_episodes, gamma):\n",
    "    episode_rewards = []\n",
    "    \n",
    "    # Use tqdm for progress tracking\n",
    "    for episode in tqdm(range(num_episodes), desc=\"Training Progress\"):\n",
    "        log_probs, rewards = run_episode(env, policy)\n",
    "        \n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns, device=device)\n",
    "        \n",
    "        loss = []\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            loss.append(-log_prob * R)\n",
    "        loss = torch.stack(loss).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        episode_rewards.append(sum(rewards))\n",
    "        \n",
    "        # Update tqdm postfix with the current average reward\n",
    "        if episode % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            tqdm.write(f'Episode {episode}, Average Reward: {avg_reward:.2f}')\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "# Main execution\n",
    "env_name = 'PongNoFrameskip-v4'\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "policy = PolicyNetwork().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "num_episodes = 1000\n",
    "gamma = 0.99\n",
    "\n",
    "rewards = train(env, policy, optimizer, num_episodes, gamma)\n",
    "\n",
    "# Plot results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.plot(np.convolve(rewards, np.ones(100)/100, mode='valid'))\n",
    "plt.title('Episode Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the trained model\n",
    "def evaluate(env, policy, num_episodes=500):\n",
    "    episode_rewards = []\n",
    "    for _ in tqdm(range(num_episodes), desc=\"Evaluation Progress\"):\n",
    "        state, _ = env.reset()\n",
    "        prev_state = None\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            curr_state = preprocess(state)\n",
    "            if prev_state is not None:\n",
    "                state_diff = curr_state - prev_state\n",
    "            else:\n",
    "                state_diff = np.zeros_like(curr_state)\n",
    "            prev_state = curr_state\n",
    "            \n",
    "            state_diff = torch.FloatTensor(state_diff).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            action_probs = policy(state_diff)\n",
    "            action = torch.argmax(action_probs).item()\n",
    "            state, reward, done, _, _ = env.step(action + 2)  # +2 to map to RIGHT, LEFT actions\n",
    "            total_reward += reward\n",
    "        episode_rewards.append(total_reward)\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "eval_rewards = evaluate(env, policy)\n",
    "print(f'Mean Reward: {np.mean(eval_rewards):.2f}')\n",
    "print(f'Standard Deviation: {np.std(eval_rewards):.2f}')\n",
    "\n",
    "plt.hist(eval_rewards, bins=50)\n",
    "plt.title('Histogram of Episode Rewards')\n",
    "plt.xlabel('Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "469_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
